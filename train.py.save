

import torch
# import torchvision
from torch.utils.data import DataLoader, random_split
import torch.functional as F
import torch.nn as nn
import torchvision.transforms as transforms
from EMP_data import EmpDataset
from Unet import UNET
import os
#hyper-parameters
BATCH_SIZE = 3
LEARNING_RATE =1e-4
NUM_EPOCHS = 2
NUM_WORKERS =1
PIN_MEMORY= False

#setting the datasets path and finally dataset
os.chdir("/home/omar/code/pytorch")
dir= os.getcwd()
image_dir = os.path.join(dir,"archive/images")
seg_dir = os.path.join(dir,"archive/segmaps")
transform = transforms.Compose([transforms.ToTensor(),transforms.Resize((256,256))])
data = EmpDataset(image_dir,seg_dir,transform=transform)

#split the data set and load the data in batch to dataloader
train_data, val_data = random_split(data,(300,165))
train_dataloder = DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True)
val_dataloder = DataLoader(dataset=val_data,batch_size=BATCH_SIZE,shuffle=False)

device=  "cuda" if torch.cuda.is_available() else "cpu"
print(device)
# setup model, loss funciton and optimizer
model = UNET(3,1)
model.to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(),lr=LEARNING_RATE)


def quick_train(dataLoader,model,lossFun,optimizer):
   
 im,seg =next(iter(dataLoader))
    seg= seg.to(device)
    im= im.to(device)
    seg = (seg>0).float()
    for epoch in range(NUM_EPOCHS):
        optimizer.zero_grad()
        # with torch.autocast(device_type=device, dtype=torch.bfloat16):
        pred = model(im)
        loss =lossFun(pred,seg)
        loss.backward()
        optimizer.step()
        print(f"epoch:{epoch}/{NUM_EPOCHS}, loss={loss.item()}")    

# this code is quick development pupose only
def training(numEpochs,dataLoader,model,lossFun,optimizer):
    for epoch in range(NUM_EPOCHS):
        for idx, (image,segment) in enumerate(train_dataloder):
            segment = (segment>0).float()
            predict = model(image)
            loss = criterion(predict,segment)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            if (idx+1)%10==0:
                print(f'epoch: {epoch}/{NUM_EPOCHS}, step:{idx}/{len(train_dataloder)}')
        

def save_checkpoint(model,path):
    torch.save(model.state_dict(),path)
def load_checkpoint(model,path):
    model.load_state_dict(torch.load(path))

def check_accuracy(model,valLoader):
    num_correct =0
    num_pixels= 0
    dice_score =0
    model.eval()
    
    for img,seg in valLoader:
        seg = (seg>0).float()
        pred =torch.sigmoid(model(img))
        pred = (pred>0.5).float()

        num_correct += (pred== seg).sum()
        num_pixels+= torch.numel(pred)
        dice_score += (2 * (pred * seg).sum()) / (
                (pred + seg).sum() + 1e-8
            )

    print(
        f"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}"
    )
    print(f"Dice score: {dice_score/len(valLoader)}")
    model.train()

if __name__=="__main__":
    quick_train(train_dataloder,model,criterion,optimizer)

    # training(NUM_EPOCHS,train_dataloder,model,criterion,optimizer)
    # save_checkpoint(model,"UNET_EMP.pth")
    # check_accuracy(model,val_dataloder)
